{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de1722c",
   "metadata": {},
   "source": [
    "## How to use Python Script in HPC Server?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfa4d1",
   "metadata": {},
   "source": [
    "### To Use python Script in HPC Server You need two files to execute your code\n",
    "#### 1. .sh file\n",
    "#### 2. .py file\n",
    "\n",
    "###### 1. .py file is the file which contains the code of your interest and we will see the use case using examples of python file later.\n",
    "###### 2. .sh file is the file which contains the commands for nodes, memory, RAM, Packages etc and we will this later too.\n",
    "######   It is a Bourne shell script. They are used in many variations of UNIX-like operating systems. They have no  \"language\" and are interpreted by your shell (interpreter of terminal commands)and if u want to know more   then paste it(https://stackoverflow.com/questions/13805295/whats-a-sh-file) to your browser.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3036af4",
   "metadata": {},
   "source": [
    "## Python Script:\n",
    "##### Here i will show you a simple python script which will be use to execute in the HPC server.\n",
    "##### From importing the libraries which are required for your interest to writing the function and calling it is totally same with the code which you execute normally in your local machine.\n",
    "#### You just need to save this code in .py extension lets say we saved this file as myscript.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def coverage_normalised(gtf_file, cov_file): \n",
    "    \"\"\"\n",
    "    This code is written by Md Abuzar Khan (Data Management supervisor)\n",
    "    Date : 30/08/2023 CSIR-IGIB, New Delhi.\n",
    "    This function takes two input file.\n",
    "    1. Human reference genome\n",
    "    2. Coverage file after processing from BAM files using bedtool\n",
    "    After taking the input firstly this function process the cov_file,\n",
    "    it converts the cov_file into .csv from .txt format.\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(cov_file, sep = '\\t', comment = '#', header = None, dtype = {0 : str})\n",
    "    df.rename(columns ={\n",
    "        df.columns[0]: 'Chromosome',\n",
    "        df.columns[1]: 'Source',\n",
    "        df.columns[2]: 'Feature',\n",
    "        df.columns[3]: 'Start',\n",
    "        df.columns[4]: 'End',\n",
    "        df.columns[5]: 'needtodrop',\n",
    "        df.columns[6]: 'neetodrop',\n",
    "        df.columns[7]: 'neetdrop',\n",
    "        df.columns[8]: 'Attributes',\n",
    "        df.columns[9]:'Depth',\n",
    "        df.columns[10]:'Mapped',\n",
    "        df.columns[11]:'Length',\n",
    "        df.columns[12]:'Coverage'\n",
    "\n",
    "    },\n",
    "              inplace=True)\n",
    "\n",
    "    df = df[['Chromosome', 'Source', 'Feature', 'Start', 'End','Attributes', 'Depth', 'Mapped', 'Length','Coverage']]\n",
    "    df = df[df['Feature'] == 'gene']\n",
    "    df['Gene_ID'] = df['Attributes'].apply(lambda x: x.split(';')[0].split('\"')[1])\n",
    "    df['total_bases'] = df['End'] - df['Start'] + 1\n",
    "\n",
    "    gtf_file = pd.read_csv(gtf_file, low_memory = False)\n",
    "    gtf_file = gtf_file[gtf_file['feature'] == 'gene']\n",
    "    gtf_file['Gene_ID'] = gtf_file['attribute'].apply(lambda x: x.split(';')[0].split('\"')[1])\n",
    "\n",
    "    df = df[['Gene_ID','Depth', 'Mapped', 'Length','Coverage','total_bases']]\n",
    "\n",
    "    merged_df = gtf_file.merge(df, on = 'Gene_ID', how = 'left').fillna(0)\n",
    "\n",
    "    merged_df['Total_Bases'] = merged_df['end']- merged_df['start'] + 1\n",
    "    merged_df = merged_df[['Gene_ID','Total_Bases','Mapped','Depth','Coverage','total_bases']]\n",
    "    merged_df['coverage_normalised'] = (merged_df['Mapped'] / merged_df['Total_Bases'])*100\n",
    "    merged_df['Depth'] = merged_df['Depth'].apply(lambda x: 0 if x < 10 else x)\n",
    "    merged_df = merged_df[['Total_Bases','Gene_ID','Mapped','total_bases','coverage_normalised','Depth']]\n",
    "    merged_df.to_csv('/lustre/abuzar.khan/dengue_bdcov_files/norm_cov_data_gene/' + cov_file.split('_')[0] + '_gene.csv')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "#to get the data from the directory\n",
    "directory = \"/lustre/user_name/path/to/workingdir\"\n",
    "txt_files = [file_name for file_name in os.listdir(directory) if file_name.endswith(\".txt\")]\n",
    "for txt_file in txt_files:\n",
    "    txt_path = os.path.join(directory, txt_file)\n",
    "\n",
    "    # Call the function\n",
    "    result_df = coverage_normalised('human_gtf.csv',txt_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e5363",
   "metadata": {},
   "source": [
    "### Our .sh File and we save this lets myshfile.sh and make sure both the files are in the same directory and if not then specify the absolute path and if you want to know more about the below code please refer this link  \n",
    "#### (https://researchcomputing.princeton.edu/support/knowledge-base/python#:~:text=This%20guide%20presents%20an%20overview%20of%20installing%20Python,are%20to%20be%20run%20on%20the%20command%20line.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe107d",
   "metadata": {},
   "source": [
    "# No lets see the .sh file \n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=covrage_norm\n",
    "#SBATCH --output=log.%j  # Standard output and error log\n",
    "#SBATCH --nodes=1\n",
    "##SBATCH --ntasks-per-node=20\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=256GB  # Memory (RAM) per node. Number followed by unit prefix.\n",
    "#SBATCH --partition=compute  # Partition/queue in which to run the job\n",
    "#SBATCH --time=1193046:27:16\n",
    "\n",
    "echo \"SLURM_JOBID=\"$SLURM_JOBID\n",
    "echo \"SLURM_JOB_NODELIST=\"$SLURM_JOB_NODELIST\n",
    "echo \"SLURM_NNODES=\"$SLURM_NNODES\n",
    "echo \"SLURMTMPDIR=\"$SLURMTMPDIR\n",
    "echo \"Date = $(date)\"\n",
    "echo \"Hostname = $(hostname -s)\"\n",
    "echo \"\"\n",
    "echo \"Number of Nodes Allocated = $SLURM_JOB_NUM_NODES\"\n",
    "echo \"Number of Tasks Allocated = $SLURM_NTASKS\"\n",
    "echo \"Number of Cores/Task Allocated = $SLURM_CPUS_PER_TASK\"\n",
    "echo \"Working Directory = $(pwd)\"\n",
    "echo \"Working Directory = \"$SLURM_SUBMIT_DIR\n",
    "\n",
    "cd /lustre/user_name/path/to/workingdirectory  # Change working directory to the desired path\n",
    "module purge\n",
    "module load anaconda3/2023.3\n",
    "conda init bash  # Initialize conda for bash\n",
    "source ~/.bashrc  # Activate conda initialization\n",
    "conda create --name my-env pandas numpy csv os # Create a new conda environment and specify your required libraries\n",
    "conda activate my-env  # Activate the created environment\n",
    "python myscript.py\n",
    "/bin/hostname | tee result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8bb16c",
   "metadata": {},
   "source": [
    "### I hope you saved both the .py and .sh file in your working directory now go to the command line and write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f40dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    " $ sbatch myshfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cabaac7",
   "metadata": {},
   "source": [
    "#### it gives you a log file and to check the status of your file just type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e504e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    " $ showq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68633365",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45241bde",
   "metadata": {},
   "source": [
    "## Thank you\n",
    "#### Md Abuzar Khan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195cf49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
